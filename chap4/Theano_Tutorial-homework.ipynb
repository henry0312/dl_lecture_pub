{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Theano Tutorial @ LTI, Carnegie Mellon University\n",
    "=======================\n",
    "\n",
    "Kazuya Kawakami, clab\n",
    "\n",
    "# Theano\n",
    "\n",
    "Theano is a Python library, like numpy, that lets you to define mathematical expressions and run them on GPU.  \n",
    "In short, Theano is the best prototyping tool!!\n",
    "\n",
    "**Pros**\n",
    "\n",
    "- **Python** : Preprocessing, Modeling, Visualization. Numpy like expression.\n",
    "- **Easy to access GPU**: You don't need to do anything!!\n",
    "- **Execution speed optimizations**: Theano can use g++ or nvcc to compile your expression graph\n",
    "- **Symbolic differentiation**: Automatic Differentiation on symbolic expressions\n",
    "- **Stability optimizations**: Theano recognize numerically unstable expressions and fix them\n",
    "- **Still growing**: Developer communitiy is active\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- **Loop** : Restrictions on how the loop interact with the rest of the graph\n",
    "- ** Goto/Recursion**: are not supprted\n",
    "\n",
    "## Contents\n",
    "0. **Tools**: You don't need to know theano at all !! ( [nolearn](https://github.com/dnouri/nolearn.git), [Pylearn2](http://deeplearning.net/software/pylearn2/), [sklearn-theano](http://sklearn-theano.github.io/auto_examples/plot_mnist_generator.html#example-plot-mnist-generator-py) ).\n",
    "\n",
    "1. **Overview**: How theano codes look like??\n",
    "2. **Variables**: Symbolic variable, Shared variable\n",
    "3. **Function, Computational Graph**: tensor.function, tensor.clone, theano.printing.pp, theano.printing.debugprint\n",
    "4. **Math**:  Comparison, Condition\n",
    "5. **Linear Algebra**\n",
    "6. **Gradient**: theano.gradient.grad, theano.gradient.hessian, theano.gradient.jacobian, update\n",
    "7. **GPU**: Data type \n",
    "8. **Linear Regression**\n",
    "9. **Multi Layer Perceptron**\n",
    "10. **Convolution**\n",
    "11. **Maxpooling**\n",
    "12. **Scan**\n",
    "13. **Recurrent Neural Networks**\n",
    "14. **Tips for debugging**\n",
    "15. **Links**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server\n",
    "\n",
    "This is a hands-on tutorial, if you sent your public key to Prasanna, you can log in to your server.\n",
    "The server has 4 GPUs. To avoid all participants uses one gpu, please specify explicitly the gpu you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu0\") #gpu1, gpu2, gpu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running command line, run your code with the following options.\n",
    "\n",
    "```bash\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu0,floatX=float32, python your_code.py\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu1,floatX=float32, python your_code.py\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu2,floatX=float32, python your_code.py\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu3,floatX=float32, python your_code.py\n",
    "```\n",
    "\n",
    "Then launch your ipython server by following this [Instruction](http://ipython.org/ipython-doc/1/interactive/public_server.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools\n",
    "\n",
    "There are a lots of tools implemented with Theano.\n",
    "For example, [nolearn](https://github.com/dnouri/nolearn.git) let you write image classification in 30 lines !! \n",
    "\n",
    "More examples and tutorials are in [dl_tutorial](https://github.com/oduerr/dl_tutorial/tree/master/lasagne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#Random Seed\n",
    "rng = numpy.random.RandomState(1234)\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "\n",
    "# mnist_x is a (n_sample, n_feature=784) matrix\n",
    "mnist_x, mnist_y = shuffle(mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Show Randomly picked Examples\n",
    "def plot_sample(x, axis):\n",
    "    img = x.reshape(28, 28)\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    \n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "for i in range(36):\n",
    "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
    "    plot_sample(mnist_x[numpy.random.randint(0,60000)], ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Theano code\n",
    "\n",
    "1. Define Symbolic/Shared variables (**Variables**)\n",
    "2. Construct a computational graph (**Math**)\n",
    "3. Compile the graph (**Function**)\n",
    "4. Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "##  Step1. Define Symbolic / Shared Variables\n",
    "x, t = T.fvector(\"x\"), T.fvector(\"t\") #input\n",
    "\n",
    "W = theano.shared(rng.uniform(low=-0.08, high=0.08, size=(5, 3)), name=\"W\") #variables that are shared over iteration: weight, bias\n",
    "b = theano.shared(numpy.zeros(3), name=\"bias\")\n",
    "\n",
    "\n",
    "## Step2. Define graph\n",
    "y = T.dot(x, W) + b\n",
    "\n",
    "cost = T.sum((y - t)**2) #Cost function\n",
    "\n",
    "gW, gb = T.grad(cost, [W, b]) # Take gradient\n",
    "\n",
    "updates =  OrderedDict({W: W-0.01*gW, b: b-0.01*gb}) # Set update expression in OrderedDict\n",
    "\n",
    "\n",
    "## Step3. Compile graph\n",
    "f = theano.function(inputs=[x, t], outputs=[cost, gW, gb], updates=updates, allow_input_downcast=True)\n",
    "\n",
    "\n",
    "## Step4. Run!!\n",
    "for epoch in range(5):\n",
    "    cost, gW, gb = f([-2., -1., 1., 2., 3.], [.4, .3, .5])\n",
    "    print(\"epoch:: %d, cost:: %.3f\"%(epoch, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "In addition to numpy.ndarray, you need to know **Symbolic Variable** and **Shared Variable**.\n",
    "\n",
    "- **Symbolic Variable** is a symbolic representation of quantities you want to use in functions. (Inputs)\n",
    "- **Shared Variable** is a variable with **storage** that is shared between functions. (Weights, Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def identity(a):\n",
    "    return a\n",
    "\n",
    "a = T.iscalar('a')\n",
    "f = theano.function([a], a)\n",
    "\n",
    "print(identity(1))\n",
    "print(f(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = theano.shared(numpy.array([0., 1., 2., 3., 4.]).astype(\"float32\"), name=\"W\")\n",
    "\n",
    "print(W)\n",
    "print(W.get_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Symbolic Variables \n",
    "a = T.iscalar(\"a\") # integer\n",
    "b = T.fscalar(\"b\") # float scalar\n",
    "\n",
    "x = T.fvector(\"x\") # float vector\n",
    "X = T.fmatrix(\"X\") # float matrix\n",
    "\n",
    "## Shared Variable, store variables on cpu/gpu memory\n",
    "W = theano.shared(numpy.array([0., 1., 2., 3., 4.]).astype(\"float32\"), name=\"W\")\n",
    "bias  = theano.shared(numpy.float32(5), name=\"bias\")\n",
    "\n",
    "# Get Value from shared variable\n",
    "print(W.get_value())\n",
    "\n",
    "## Define symbolic graph\n",
    "c = a + b\n",
    "y = T.dot(x, W) + bias\n",
    "\n",
    "## Print symbolic graph\n",
    "print(theano.pp(y))\n",
    "\n",
    "##  Advanced:: You can replace some parts of computation graph with different variable\n",
    "d = theano.clone(output=c, replace={b: y}) #replace \"b\" with \"y\"\n",
    "print(theano.pp(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Function\n",
    "\n",
    "Compile symbolic graph into a function\n",
    "\n",
    "** BE CAREFUL**:: YOU NEED TO USE int32 or float32, only computations with float32 data-type can be accelerated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Compile symbplic graph into callable functions\n",
    "add = theano.function(inputs=[a, b], outputs=c)\n",
    "linear = theano.function(inputs=[x], outputs=y)\n",
    "\n",
    "## Call Functions\n",
    "print(add(1, 5))\n",
    "print(linear([0., 0., 0., 0., 1.]).astype(\"float32\"))\n",
    "\n",
    "##Print function\n",
    "theano.printing.debugprint(linear)\n",
    "\n",
    "## Advanced :: You can evaluate symbolic graph without compilation\n",
    "print(c.eval({\n",
    "            a : numpy.int32(16), \n",
    "            b : numpy.float32(12.10)\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math\n",
    "\n",
    "Most mathmatic operations are similar to numpy. However, comparison operations have special form. [[Basics](http://deeplearning.net/software/theano/library/tensor/basic.html)]\n",
    "\n",
    " Condition operation is \n",
    " ```python\n",
    " T.switch(condition, if true, if false)\n",
    " ```\n",
    " \n",
    " Comparison is \n",
    " \n",
    " ```python\n",
    " T.gt(a, b) #Greater Than\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.fvector(\"x\")\n",
    "\n",
    "### Basic Math operation & Activation funcsions\n",
    "exp_x = T.exp(x)\n",
    "sigmoid_x = T.nnet.sigmoid(x)\n",
    "tanh_x = T.tanh(x)\n",
    "\n",
    "### Advanced:: condition and comparison\n",
    "relu_x = T.switch(T.gt(x, 0), x, 0)\n",
    "\n",
    "f = theano.function([x], [exp_x, sigmoid_x, tanh_x, relu_x])\n",
    "f(numpy.array([-2., -1., 1., 2., 3.]).astype(\"float32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient (Automatic Differentiation)\n",
    "\n",
    "You can define gradient symbolically. Amazing!!\n",
    "\n",
    "If you want to use Jacobian or Hessian, use theano.gradient.jacobian,  theano.gradient.hessian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y = x ** 2\n",
    "x = T.fscalar(\"x\")\n",
    "y = x ** 2\n",
    "gy = theano.grad(cost=y, wrt=x) ## 2x\n",
    "\n",
    "f = theano.function([x], [y, gy]) ## x**2, 2x\n",
    "print(f(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Updateing your parameters (shared variables)** is the key process, but it's a bit complicated.\n",
    "\n",
    "Let's start from a toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(5):\n",
    "    print(a)\n",
    "    a += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Define a function which update t by 1 for each call.\n",
    "\n",
    "t = theano.shared(numpy.int32(0))\n",
    "increment = theano.function([], t, updates=OrderedDict({t: t+1}) ) #OrderedDict({before update: after update})\n",
    "for i in range(5):\n",
    "    t = increment()\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "from collections import OrderedDict\n",
    "rng = numpy.random.RandomState(1234)\n",
    "\n",
    "##  Step1. Define Symbolic / Shared Variables\n",
    "x, t = T.fvector(\"x\"), T.fvector(\"t\") #inputs\n",
    "\n",
    "W = theano.shared(rng.uniform(low=-0.08, high=0.08, size=(5, 3)), name=\"W\") #variables that are shared over iterations\n",
    "b = theano.shared(numpy.zeros(3), name=\"bias\")\n",
    "\n",
    "\n",
    "## Step2. Define graph\n",
    "y = T.dot(x, W) + b\n",
    "#y = T.nnet.sigmoid(T.dot(x, W) + b)\n",
    "#y = T.tanh(T.dot(x, W) + b)\n",
    "cost = T.sum((y - t)**2)\n",
    "\n",
    "\n",
    "gW, gb = T.grad(cost, [W, b]) # Take gradient\n",
    "\n",
    "updates =  OrderedDict({W: W-0.01*gW, b: b-0.01*gb}) # Set update expression in OrderedDict\n",
    "\n",
    "\n",
    "## Step3. Compile graph\n",
    "f = theano.function(inputs=[x, t], outputs=[cost, gW, gb], updates=updates, allow_input_downcast=True)\n",
    "\n",
    "## Step4. Run!!\n",
    "for epoch in range(5):\n",
    "    cost, gW, gb = f([-2., -1., 1., 2., 3.], [.4, .3, .5])\n",
    "    print(\"epoch:: %d, cost:: %.3f\"%(epoch, cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since we have Autodiff, it's easy to change linear regression to non-linear regression.** Try it!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron (MLP) \n",
    "\n",
    "<img src=\"http://k-kawakami.com/img/mlp.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(mnist_x, mnist_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Multi Layer Perceptron\n",
    "rng = numpy.random.RandomState(1234)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, in_dim, out_dim, function):\n",
    "        self.W = theano.shared(\n",
    "            rng.uniform(low=-0.08, high=0.08, size=(in_dim, out_dim)).astype('float32'),\n",
    "            name='W'\n",
    "        )\n",
    "        self.b = theano.shared(numpy.zeros(out_dim).astype('float32'), name='bias')\n",
    "        self.func = function\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # 前回のパラメータ更新量 (P.52)\n",
    "        self.prev_dW = theano.shared(numpy.zeros((in_dim, out_dim)).astype('float32'), name='prev_dW')\n",
    "        self.prev_db = theano.shared(numpy.zeros(out_dim).astype('float32'), name='prev_db')\n",
    "        self.prev_dparams = [self.prev_dW, self.prev_db]\n",
    "\n",
    "    def fprop(self, x):\n",
    "        h = self.func(T.dot(x, self.W) + self.b)\n",
    "        self.h = h # hは教科書のz\n",
    "        return h\n",
    "    \n",
    "    def predict(self, x):\n",
    "        return self.fprop(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprops(layers, x):\n",
    "    '''\n",
    "    ネットワーク全体の順伝播\n",
    "\n",
    "    :param layers: ネットワーク \n",
    "    :param x: 入力\n",
    "    :return: 出力層の出力\n",
    "    '''\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.fprop(z)    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost Function (Negative Log Likelihood)\n",
    "def cross_entropy(y, d):\n",
    "    '''\n",
    "    交差エントロピーを計算する\n",
    "    See: (2.11) and #3.3\n",
    "\n",
    "    :param y: 出力層の出力\n",
    "    :param d: 目標出力\n",
    "    :return: 交差エントロピー\n",
    "    '''\n",
    "    # cf. http://deeplearning.net/tutorial/logreg.html#defining-a-loss-function\n",
    "    # cf. http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n",
    "    return -T.mean(T.log(y)[T.arange(d.shape[0]), d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd(params, gparams, eps=0.01):\n",
    "    '''\n",
    "    stochastic gradient descent\n",
    "    See: #3.2 and P.52\n",
    "    \n",
    "    :param params: 更新するパラメータ\n",
    "    :param gparams: パラメータの勾配\n",
    "    :param eps: 学習率\n",
    "    :return: 更新後のパラメータが格納されたOrderedDict\n",
    "    '''\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam in zip(params, gparams):\n",
    "        updates[param] = param - eps * gparam\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_with_momentum(params, gparams, prev_gparams, eps=0.01, mu=0.9):\n",
    "    '''\n",
    "    stochastic gradient descent with momentum\n",
    "    See: #3.2, #3.6.4 and P.52\n",
    "    \n",
    "    :param params: 更新するパラメータ\n",
    "    :param gparams: パラメータの勾配\n",
    "    :param eps: 学習率\n",
    "    :param mu: モメンタム\n",
    "    :return: 更新後のパラメータが格納されたOrderedDict\n",
    "    '''\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam, prev_gparam in zip(params, gparams, prev_gparams):\n",
    "        updates[param] = param - eps * gparam + mu * prev_gparam\n",
    "        updates[prev_gparam] = - eps * gparam + mu * prev_gparam\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(layers, x):\n",
    "    '''\n",
    "    学習済みネットワークを用いて推定する（ネットワーク全体の順伝播）\n",
    "\n",
    "    :param layers: ネットワーク \n",
    "    :param x: 入力\n",
    "    :return: 推定値（出力層の出力）\n",
    "    '''\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.predict(z)    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "activation = T.nnet.sigmoid #T.tanh\n",
    "\n",
    "layers = [\n",
    "    Layer(in_dim=784, out_dim=500, function=activation),\n",
    "    Layer(in_dim=500, out_dim=500, function=activation),\n",
    "    Layer(in_dim=500, out_dim=500, function=activation),\n",
    "    Layer(in_dim=500, out_dim=10, function=T.nnet.softmax),\n",
    "]\n",
    "\n",
    "## Collect Parameters and Symbolic output\n",
    "params = []\n",
    "prev_dparams = []\n",
    "for layer in layers:\n",
    "    params += layer.params\n",
    "    prev_dparams += layer.prev_dparams\n",
    "\n",
    "y = fprops(layers, x)\n",
    "cost = cross_entropy(y, t)\n",
    "\n",
    "## Get Gradient\n",
    "gparams = T.grad(cost, params)\n",
    "#updates = sgd(params, gparams)\n",
    "updates = sgd_with_momentum(params, gparams, prev_dparams)\n",
    "\n",
    "## Compile \n",
    "train = theano.function([x,t], cost, updates=updates)\n",
    "test = theano.function([x,t],[cost, T.argmax(y, axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.174, Validation F1:: 0.223\n",
      "EPOCH::  10, Validatioon Cost:: 0.332, Validation F1:: 0.904\n",
      "EPOCH::  20, Validatioon Cost:: 0.230, Validation F1:: 0.934\n",
      "EPOCH::  30, Validatioon Cost:: 0.165, Validation F1:: 0.953\n",
      "EPOCH::  40, Validatioon Cost:: 0.132, Validation F1:: 0.961\n",
      "EPOCH::  50, Validatioon Cost:: 0.116, Validation F1:: 0.966\n",
      "EPOCH::  60, Validatioon Cost:: 0.104, Validation F1:: 0.970\n",
      "EPOCH::  70, Validatioon Cost:: 0.102, Validation F1:: 0.969\n",
      "EPOCH::  80, Validatioon Cost:: 0.099, Validation F1:: 0.974\n",
      "EPOCH::  90, Validatioon Cost:: 0.092, Validation F1:: 0.975\n",
      "EPOCH:: 100, Validatioon Cost:: 0.093, Validation F1:: 0.975\n",
      "EPOCH:: 110, Validatioon Cost:: 0.094, Validation F1:: 0.976\n",
      "EPOCH:: 120, Validatioon Cost:: 0.096, Validation F1:: 0.977\n",
      "EPOCH:: 130, Validatioon Cost:: 0.098, Validation F1:: 0.976\n",
      "EPOCH:: 140, Validatioon Cost:: 0.099, Validation F1:: 0.977\n",
      "EPOCH:: 150, Validatioon Cost:: 0.101, Validation F1:: 0.977\n",
      "EPOCH:: 160, Validatioon Cost:: 0.103, Validation F1:: 0.977\n",
      "EPOCH:: 170, Validatioon Cost:: 0.105, Validation F1:: 0.977\n",
      "EPOCH:: 180, Validatioon Cost:: 0.107, Validation F1:: 0.977\n",
      "EPOCH:: 190, Validatioon Cost:: 0.107, Validation F1:: 0.978\n",
      "EPOCH:: 200, Validatioon Cost:: 0.109, Validation F1:: 0.977\n",
      "EPOCH:: 210, Validatioon Cost:: 0.109, Validation F1:: 0.977\n",
      "EPOCH:: 220, Validatioon Cost:: 0.111, Validation F1:: 0.977\n",
      "EPOCH:: 230, Validatioon Cost:: 0.112, Validation F1:: 0.978\n",
      "EPOCH:: 240, Validatioon Cost:: 0.113, Validation F1:: 0.977\n",
      "EPOCH:: 250, Validatioon Cost:: 0.114, Validation F1:: 0.977\n",
      "EPOCH:: 260, Validatioon Cost:: 0.114, Validation F1:: 0.978\n",
      "EPOCH:: 270, Validatioon Cost:: 0.115, Validation F1:: 0.978\n",
      "EPOCH:: 280, Validatioon Cost:: 0.116, Validation F1:: 0.978\n",
      "EPOCH:: 290, Validatioon Cost:: 0.117, Validation F1:: 0.978\n",
      "EPOCH:: 300, Validatioon Cost:: 0.118, Validation F1:: 0.978\n",
      "EPOCH:: 310, Validatioon Cost:: 0.118, Validation F1:: 0.978\n",
      "EPOCH:: 320, Validatioon Cost:: 0.119, Validation F1:: 0.978\n",
      "EPOCH:: 330, Validatioon Cost:: 0.119, Validation F1:: 0.978\n",
      "EPOCH:: 340, Validatioon Cost:: 0.120, Validation F1:: 0.978\n",
      "EPOCH:: 350, Validatioon Cost:: 0.120, Validation F1:: 0.978\n",
      "EPOCH:: 360, Validatioon Cost:: 0.121, Validation F1:: 0.978\n",
      "EPOCH:: 370, Validatioon Cost:: 0.121, Validation F1:: 0.978\n",
      "EPOCH:: 380, Validatioon Cost:: 0.122, Validation F1:: 0.978\n",
      "EPOCH:: 390, Validatioon Cost:: 0.123, Validation F1:: 0.978\n",
      "EPOCH:: 400, Validatioon Cost:: 0.123, Validation F1:: 0.978\n",
      "EPOCH:: 410, Validatioon Cost:: 0.123, Validation F1:: 0.978\n",
      "EPOCH:: 420, Validatioon Cost:: 0.124, Validation F1:: 0.978\n",
      "EPOCH:: 430, Validatioon Cost:: 0.124, Validation F1:: 0.978\n",
      "EPOCH:: 440, Validatioon Cost:: 0.124, Validation F1:: 0.978\n",
      "EPOCH:: 450, Validatioon Cost:: 0.125, Validation F1:: 0.978\n",
      "EPOCH:: 460, Validatioon Cost:: 0.125, Validation F1:: 0.978\n",
      "EPOCH:: 470, Validatioon Cost:: 0.126, Validation F1:: 0.978\n",
      "EPOCH:: 480, Validatioon Cost:: 0.126, Validation F1:: 0.978\n",
      "EPOCH:: 490, Validatioon Cost:: 0.126, Validation F1:: 0.978\n",
      "EPOCH:: 500, Validatioon Cost:: 0.127, Validation F1:: 0.978\n"
     ]
    }
   ],
   "source": [
    "## Iterate\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.tensor.shared_randomstreams import RandomStreams\n",
    "trng = RandomStreams(1234)\n",
    "\n",
    "class Dropout(Layer):\n",
    "    def __init__(self, in_dim, out_dim, function, p):\n",
    "        super().__init__(in_dim, out_dim, function)\n",
    "        self.p = numpy.float32(p)\n",
    "\n",
    "    def fprop(self, x, use_noise=True):\n",
    "        mask = trng.binomial(size=x.shape, n=1, p=self.p).astype('float32') # int32だとtheano.functionでコンパイル出来ない:(\n",
    "        h = self.func(T.dot(x*mask, self.W) + self.b) # TODO: check bias\n",
    "        self.h = h\n",
    "        return h\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # #3.5.3\n",
    "        h = self.func(T.dot(x, self.p*self.W) + self.b)\n",
    "        self.h = h\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "activation = T.nnet.sigmoid #T.tanh\n",
    "\n",
    "layers = [\n",
    "    Dropout(in_dim=784, out_dim=1000, function=activation, p=0.9),\n",
    "    Dropout(in_dim=1000, out_dim=1000, function=activation, p=0.5),\n",
    "    Dropout(in_dim=1000, out_dim=1000, function=activation, p=0.5),\n",
    "    Layer(in_dim=1000, out_dim=10, function=T.nnet.softmax),\n",
    "]\n",
    "\n",
    "## Collect Parameters and Symbolic output\n",
    "params = []\n",
    "prev_dparams = []\n",
    "for layer in layers:\n",
    "    params += layer.params\n",
    "    prev_dparams += layer.prev_dparams\n",
    "\n",
    "y = fprops(layers, x)\n",
    "cost = cross_entropy(y, t)\n",
    "\n",
    "## Get Gradient\n",
    "gparams = T.grad(cost, params)\n",
    "#updates = sgd(params, gparams)\n",
    "updates = sgd_with_momentum(params, gparams, prev_dparams)\n",
    "\n",
    "## Compile \n",
    "train = theano.function([x,t], cost, updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = predict(layers, x)\n",
    "cost = cross_entropy(y, t)\n",
    "\n",
    "## Compile \n",
    "test = theano.function([x,t],[cost, T.argmax(y, axis=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.280, Validation F1:: 0.115\n",
      "EPOCH::  10, Validatioon Cost:: 0.508, Validation F1:: 0.848\n",
      "EPOCH::  20, Validatioon Cost:: 0.375, Validation F1:: 0.891\n",
      "EPOCH::  30, Validatioon Cost:: 0.333, Validation F1:: 0.901\n",
      "EPOCH::  40, Validatioon Cost:: 0.292, Validation F1:: 0.916\n",
      "EPOCH::  50, Validatioon Cost:: 0.261, Validation F1:: 0.923\n",
      "EPOCH::  60, Validatioon Cost:: 0.235, Validation F1:: 0.929\n",
      "EPOCH::  70, Validatioon Cost:: 0.216, Validation F1:: 0.934\n",
      "EPOCH::  80, Validatioon Cost:: 0.201, Validation F1:: 0.940\n",
      "EPOCH::  90, Validatioon Cost:: 0.191, Validation F1:: 0.941\n",
      "EPOCH:: 100, Validatioon Cost:: 0.173, Validation F1:: 0.947\n",
      "EPOCH:: 110, Validatioon Cost:: 0.164, Validation F1:: 0.950\n",
      "EPOCH:: 120, Validatioon Cost:: 0.156, Validation F1:: 0.952\n",
      "EPOCH:: 130, Validatioon Cost:: 0.148, Validation F1:: 0.955\n",
      "EPOCH:: 140, Validatioon Cost:: 0.141, Validation F1:: 0.956\n",
      "EPOCH:: 150, Validatioon Cost:: 0.136, Validation F1:: 0.959\n",
      "EPOCH:: 160, Validatioon Cost:: 0.130, Validation F1:: 0.960\n",
      "EPOCH:: 170, Validatioon Cost:: 0.125, Validation F1:: 0.961\n",
      "EPOCH:: 180, Validatioon Cost:: 0.123, Validation F1:: 0.962\n",
      "EPOCH:: 190, Validatioon Cost:: 0.115, Validation F1:: 0.965\n",
      "EPOCH:: 200, Validatioon Cost:: 0.114, Validation F1:: 0.964\n",
      "EPOCH:: 210, Validatioon Cost:: 0.113, Validation F1:: 0.964\n",
      "EPOCH:: 220, Validatioon Cost:: 0.109, Validation F1:: 0.966\n",
      "EPOCH:: 230, Validatioon Cost:: 0.105, Validation F1:: 0.967\n",
      "EPOCH:: 240, Validatioon Cost:: 0.105, Validation F1:: 0.967\n",
      "EPOCH:: 250, Validatioon Cost:: 0.100, Validation F1:: 0.969\n",
      "EPOCH:: 260, Validatioon Cost:: 0.096, Validation F1:: 0.970\n",
      "EPOCH:: 270, Validatioon Cost:: 0.095, Validation F1:: 0.971\n",
      "EPOCH:: 280, Validatioon Cost:: 0.094, Validation F1:: 0.971\n",
      "EPOCH:: 290, Validatioon Cost:: 0.092, Validation F1:: 0.972\n",
      "EPOCH:: 300, Validatioon Cost:: 0.089, Validation F1:: 0.973\n",
      "EPOCH:: 310, Validatioon Cost:: 0.089, Validation F1:: 0.972\n",
      "EPOCH:: 320, Validatioon Cost:: 0.086, Validation F1:: 0.973\n",
      "EPOCH:: 330, Validatioon Cost:: 0.085, Validation F1:: 0.973\n",
      "EPOCH:: 340, Validatioon Cost:: 0.085, Validation F1:: 0.973\n",
      "EPOCH:: 350, Validatioon Cost:: 0.083, Validation F1:: 0.975\n",
      "EPOCH:: 360, Validatioon Cost:: 0.081, Validation F1:: 0.975\n",
      "EPOCH:: 370, Validatioon Cost:: 0.082, Validation F1:: 0.974\n",
      "EPOCH:: 380, Validatioon Cost:: 0.079, Validation F1:: 0.975\n",
      "EPOCH:: 390, Validatioon Cost:: 0.077, Validation F1:: 0.976\n",
      "EPOCH:: 400, Validatioon Cost:: 0.076, Validation F1:: 0.976\n",
      "EPOCH:: 410, Validatioon Cost:: 0.076, Validation F1:: 0.976\n",
      "EPOCH:: 420, Validatioon Cost:: 0.075, Validation F1:: 0.977\n",
      "EPOCH:: 430, Validatioon Cost:: 0.076, Validation F1:: 0.976\n",
      "EPOCH:: 440, Validatioon Cost:: 0.073, Validation F1:: 0.978\n",
      "EPOCH:: 450, Validatioon Cost:: 0.072, Validation F1:: 0.978\n",
      "EPOCH:: 460, Validatioon Cost:: 0.073, Validation F1:: 0.978\n",
      "EPOCH:: 470, Validatioon Cost:: 0.073, Validation F1:: 0.978\n",
      "EPOCH:: 480, Validatioon Cost:: 0.071, Validation F1:: 0.979\n",
      "EPOCH:: 490, Validatioon Cost:: 0.070, Validation F1:: 0.979\n",
      "EPOCH:: 500, Validatioon Cost:: 0.071, Validation F1:: 0.978\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises: Squeeze out the last few percent!!\n",
    "1. Try other objectives, MSE, Cross Entropy. \n",
    "2. Try symmetric activation functions (tanh, softsign).\n",
    "3. Implement ReLU (Hint. look at Math section).\n",
    "4. Add Dropout (Hint. apply random mask to inputs in fprop).\n",
    "5. Add learning rate adjuster (Hint. make learning rate as shared variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Theano Documentation](http://deeplearning.net/software/theano/introduction.html)\n",
    "2. [Deep Learning Tutorial](http://deeplearning.net/tutorial/)\n",
    "3. F. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. Goodfellow, A. Bergeron, N. Bouchard, D. Warde-Farley and Y. Bengio. [Theano: new features nd speed improvements\"](http://arxiv.org/pdf/1211.5590.pdf). NIPS 2012 deep learning workshop.\n",
    "4. J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley and Y. Bengio. [Theano: A CPU and GPU Math Expression Compiler](http://www.iro.umontreal.ca/~lisa/pointeurs/theano_scipy2010.pdf). Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June 30 - July 3, Austin, TX\n",
    "5. [Wikipedia, Convolution](https://en.wikipedia.org/wiki/Convolution)\n",
    "6. [Stanford UFLDL Tutorial](http://ufldl.stanford.edu/wiki/index.php/UFLDL_Tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Acknowledgements\n",
    "\n",
    "Thank you Guillaume Lample, Ramón Fernandez Astudillo, Chu-Cheng Lin for reviewing this tutorial. \n",
    "\n",
    "And I appreciate Theano developers!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
