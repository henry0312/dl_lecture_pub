{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization\n",
    "\n",
    "参考\n",
    "\n",
    "* [『深層学習』（岡谷貴之）：機械学習プロフェッショナルシリーズ｜講談社BOOK倶楽部](http://bookclub.kodansha.co.jp/product?isbn=9784061529021 \"『深層学習』（岡谷貴之）：機械学習プロフェッショナルシリーズ｜講談社BOOK倶楽部\")\n",
    "* [深層学習 | 近代科学社](http://www.kindaikagaku.co.jp/information/kd0487.htm \"深層学習\")\n",
    "* [機械学習 - ニューラルネットで最適化アルゴリズムを色々試してみる - Qiita](http://qiita.com/hogefugabar/items/1d4f6c905d0edbc71af2 \"機械学習 - ニューラルネットで最適化アルゴリズムを色々試してみる - Qiita\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 3: GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Random Seed\n",
    "rng = numpy.random.RandomState(1234)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = shuffle(mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, in_dim, out_dim, function, Optimizer, **args):\n",
    "        '''\n",
    "        :param in_dim: 入力次元\n",
    "        :param out_dim: 出力次元\n",
    "        :param function: 活性化関数\n",
    "        :param Optimizer: 最適化手法（クラスを与える）\n",
    "        :param args: Optimizerに渡すパラメータ\n",
    "        '''\n",
    "        self.func = function\n",
    "        self.W = theano.shared(\n",
    "            rng.uniform(\n",
    "                low=-numpy.sqrt(6./(in_dim+out_dim)),\n",
    "                high=numpy.sqrt(6./(in_dim+out_dim)),\n",
    "                size=(in_dim, out_dim)\n",
    "            ).astype('float32'), name='W'\n",
    "        )\n",
    "        self.b = theano.shared(numpy.zeros(out_dim).astype('float32'), name='bias')\n",
    "        self.params = [self.W, self.b]\n",
    "        self.optimizer = Optimizer(self.params, **args)\n",
    "\n",
    "    def fprop(self, x):\n",
    "        '''\n",
    "        順伝播\n",
    "\n",
    "        :param x: 入力\n",
    "        :return: レイヤーの出力\n",
    "        '''\n",
    "        z = self.func(T.dot(x, self.W) + self.b)\n",
    "        self.z = z\n",
    "        return z\n",
    "    \n",
    "    def optimize(self, cost):\n",
    "        '''\n",
    "        パラメータの最適化\n",
    "        See:\n",
    "          http://deeplearning.net/software/theano/library/compile/function.html\n",
    "\n",
    "        :param cost: コスト\n",
    "        :return: Tuple(パラメータ，新しい値)のリスト\n",
    "        '''\n",
    "        updates = self.optimizer.update(cost)\n",
    "        return list(updates.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params):\n",
    "        '''\n",
    "        :param params: 更新するパラメータ\n",
    "        '''\n",
    "        self.params = params\n",
    "    \n",
    "    def update(self, cost):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, params, eps=0.01):\n",
    "        '''\n",
    "        :param params: 更新するパラメータ\n",
    "        :param eps: 学習率\n",
    "        '''\n",
    "        super().__init__(params)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def update(self, cost):\n",
    "        '''\n",
    "        :param cost: コスト\n",
    "        :return: 更新後のパラメータが格納されたOrderedDict\n",
    "        '''\n",
    "        updates = OrderedDict()\n",
    "\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates[param] = param - self.eps * gparam\n",
    "\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Momentum(SGD):\n",
    "    def __init__(self, params, eps=0.01, mu=0.9):\n",
    "        '''\n",
    "        :param params: 更新するパラメータ\n",
    "        :param eps: 学習率\n",
    "        :param mu: モメンタム\n",
    "        '''\n",
    "        SGD.__init__(self, params, eps)\n",
    "        #super().__init__(eps) # http://stackoverflow.com/questions/222877/how-to-use-super-in-python\n",
    "        self.mu = mu\n",
    "        self.prev_gparams = [\n",
    "            theano.shared(numpy.zeros(param.shape.eval()).astype('float32')) for param in params\n",
    "        ]\n",
    "    \n",
    "    def update(self, cost):\n",
    "        '''\n",
    "        :param cost: コスト\n",
    "        :return: 更新後のパラメータが格納されたOrderedDict\n",
    "        '''\n",
    "        updates = OrderedDict()\n",
    "\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        for param, gparam, prev_gparam in zip(self.params, gparams, self.prev_gparams):\n",
    "            updates[param] = param - self.eps * gparam + self.mu * prev_gparam\n",
    "            updates[prev_gparam] = - self.eps * gparam + self.mu * prev_gparam\n",
    "\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad\n",
    "\n",
    "* [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](http://jmlr.org/papers/v12/duchi11a.html \"Adaptive Subgradient Methods for Online Learning and Stochastic Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdaGrad(Optimizer):\n",
    "    def __init__(self, params, gamma=1.0, eps=1e-6):\n",
    "        '''\n",
    "        :param params: 更新するパラメータ\n",
    "        :param gamma: 学習率の計算で利用する定数\n",
    "        :param eps: パラメータが発散するのを防ぐために使う微少な値\n",
    "        '''\n",
    "        super().__init__(params)\n",
    "        self.gamma = numpy.float32(gamma)\n",
    "        self.eps = numpy.float32(eps)\n",
    "        self.grads = [\n",
    "            theano.shared(numpy.zeros(param.shape.eval()).astype('float32')) for param in self.params\n",
    "        ]\n",
    "    \n",
    "    def update(self, cost):\n",
    "        '''\n",
    "        :param cost: コスト\n",
    "        :return: 更新後のパラメータが格納されたOrderedDict\n",
    "        '''\n",
    "        updates = OrderedDict()\n",
    "\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        for param, gparam, grad in zip(self.params, gparams, self.grads):\n",
    "            tau = self.gamma / (T.sqrt(grad + gparam * gparam) + self.eps)\n",
    "            updates[param] = param - tau * gparam\n",
    "            updates[grad] = grad + gparam * gparam\n",
    "\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "* [Adam: A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980 \"Adam: A Method for Stochastic Optimization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "    def __init__(self, params, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8, tau=1.0-1e-8):\n",
    "        '''\n",
    "        :param params: 更新するパラメータ\n",
    "        :param alpha:\n",
    "        :param beta1:\n",
    "        :param beta2:\n",
    "        :param eps: パラメータが発散するのを防ぐために使う微少な値\n",
    "        :param tau:\n",
    "        '''\n",
    "        self.params = params\n",
    "        self.m = [\n",
    "            theano.shared(numpy.zeros(param.shape.eval()).astype('float32'), name=\"mean\") for param in self.params\n",
    "        ]\n",
    "        self.v = [\n",
    "            theano.shared(numpy.zeros(param.shape.eval()).astype('float32'), name=\"variance\") for param in self.params\n",
    "        ]\n",
    "        self.alpha = numpy.float32(alpha)\n",
    "        self.beta1 = theano.shared(numpy.float32(beta1), name=\"beta1\")\n",
    "        self.beta2 = numpy.float32(beta2)\n",
    "        self.eps = numpy.float32(eps)\n",
    "        self.tau = numpy.float32(tau)\n",
    "    \n",
    "    def update(self, cost):\n",
    "        '''\n",
    "        :param cost: コスト\n",
    "        :return: 更新後のパラメータが格納されたOrderedDict\n",
    "        '''\n",
    "        updates = OrderedDict()\n",
    "\n",
    "        gparams = T.grad(cost, self.params)\n",
    "        beta1 = self.beta1 * self.tau\n",
    "        for param, gparam, m, v in zip(self.params, gparams, self.m, self.v):\n",
    "            new_m = beta1 * m + (numpy.float32(1.0) - beta1) * gparam\n",
    "            new_v = self.beta2 * v + (numpy.float32(1.0) - self.beta2) * gparam * gparam\n",
    "            m_hat = new_m / (numpy.float32(1.0) - beta1)\n",
    "            v_hat = new_v / (numpy.float32(1.0) - self.beta2)\n",
    "            updates[param] = param - self.alpha * m_hat / (T.sqrt(v_hat) + self.eps)\n",
    "            updates[m] = new_m\n",
    "            updates[v] = new_v\n",
    "        updates[self.beta1] = beta1\n",
    "        \n",
    "        return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano functionをコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprops(layers, x):\n",
    "    '''\n",
    "    ネットワーク全体の順伝播\n",
    "\n",
    "    :param layers: ネットワーク \n",
    "    :param x: 入力\n",
    "    :return: 出力層の出力\n",
    "    '''\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.fprop(z)    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost Function (Negative Log Likelihood)\n",
    "def cross_entropy(y, d):\n",
    "    '''\n",
    "    交差エントロピーを計算する\n",
    "\n",
    "    :param y: 出力層の出力\n",
    "    :param d: 目標出力\n",
    "    :return: 交差エントロピー\n",
    "    '''\n",
    "    # cf. http://deeplearning.net/tutorial/logreg.html#defining-a-loss-function\n",
    "    # cf. http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n",
    "    return -T.mean(T.log(y)[T.arange(d.shape[0]), d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_functions(layers):\n",
    "    '''\n",
    "    訓練とテストに利用するtheano.functionをコンパイルする\n",
    "\n",
    "    :param layers: 学習対象のネットワーク\n",
    "    :return: 訓練とテストに利用するtheano.function\n",
    "    '''\n",
    "    x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "    \n",
    "    y = fprops(layers, x)\n",
    "    cost = cross_entropy(y, t)\n",
    "    \n",
    "    updates = []\n",
    "    for layer in layers:\n",
    "        updates += layer.optimize(cost)\n",
    "    \n",
    "    ## Compile\n",
    "    train = theano.function([x,t], cost, updates=updates)\n",
    "    test = theano.function([x,t],[cost, T.argmax(y, axis=1)])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(mnist_x, mnist_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.306, Validation F1:: 0.098\n",
      "EPOCH::  10, Validatioon Cost:: 2.306, Validation F1:: 0.113\n",
      "EPOCH::  20, Validatioon Cost:: 2.310, Validation F1:: 0.104\n",
      "EPOCH::  30, Validatioon Cost:: 2.305, Validation F1:: 0.113\n",
      "EPOCH::  40, Validatioon Cost:: 2.304, Validation F1:: 0.094\n",
      "EPOCH::  50, Validatioon Cost:: 2.303, Validation F1:: 0.094\n",
      "EPOCH::  60, Validatioon Cost:: 2.306, Validation F1:: 0.113\n",
      "EPOCH::  70, Validatioon Cost:: 2.306, Validation F1:: 0.113\n",
      "EPOCH::  80, Validatioon Cost:: 2.304, Validation F1:: 0.104\n",
      "EPOCH::  90, Validatioon Cost:: 2.304, Validation F1:: 0.100\n",
      "EPOCH:: 100, Validatioon Cost:: 2.303, Validation F1:: 0.104\n",
      "EPOCH:: 110, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 120, Validatioon Cost:: 2.303, Validation F1:: 0.103\n",
      "EPOCH:: 130, Validatioon Cost:: 2.303, Validation F1:: 0.098\n",
      "EPOCH:: 140, Validatioon Cost:: 2.304, Validation F1:: 0.113\n",
      "EPOCH:: 150, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 160, Validatioon Cost:: 2.305, Validation F1:: 0.094\n",
      "EPOCH:: 170, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 180, Validatioon Cost:: 2.305, Validation F1:: 0.094\n",
      "EPOCH:: 190, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 200, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 210, Validatioon Cost:: 2.303, Validation F1:: 0.094\n",
      "EPOCH:: 220, Validatioon Cost:: 2.303, Validation F1:: 0.113\n",
      "EPOCH:: 230, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 240, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 250, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 260, Validatioon Cost:: 2.303, Validation F1:: 0.113\n",
      "EPOCH:: 270, Validatioon Cost:: 2.303, Validation F1:: 0.094\n",
      "EPOCH:: 280, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 290, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 300, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 310, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 320, Validatioon Cost:: 2.302, Validation F1:: 0.103\n",
      "EPOCH:: 330, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 340, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 350, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 360, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 370, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 380, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 390, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 400, Validatioon Cost:: 2.301, Validation F1:: 0.103\n",
      "EPOCH:: 410, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 420, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 430, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 440, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 450, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 460, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 470, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 480, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 490, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 500, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 510, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 520, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 530, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 540, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 550, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 560, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 570, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 580, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 590, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 600, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 610, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 620, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 630, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 640, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 650, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 660, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 670, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 680, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 690, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 700, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 710, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 720, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 730, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 740, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 750, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 760, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 770, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 780, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 790, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 800, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 810, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 820, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 830, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 840, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 850, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 860, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 870, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 880, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 890, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 900, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 910, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 920, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 930, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 940, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 950, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 960, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 970, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 980, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 990, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 1000, Validatioon Cost:: 2.301, Validation F1:: 0.113\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=SGD),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax, Optimizer=SGD),\n",
    "]\n",
    "\n",
    "train, test = compile_functions(layers)\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.330, Validation F1:: 0.104\n",
      "EPOCH::  10, Validatioon Cost:: 2.303, Validation F1:: 0.104\n",
      "EPOCH::  20, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH::  30, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH::  40, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH::  50, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH::  60, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH::  70, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH::  80, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH::  90, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 100, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 110, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 120, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 130, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 140, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 150, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 160, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 170, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 180, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 190, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 200, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 210, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 220, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 230, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 240, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 250, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 260, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 270, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 280, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 290, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 300, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 310, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 320, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 330, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 340, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 350, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 360, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 370, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 380, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 390, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 400, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 410, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 420, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 430, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 440, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 450, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 460, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 470, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 480, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 490, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 500, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 510, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 520, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 530, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 540, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 550, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 560, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 570, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 580, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 590, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 600, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 610, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 620, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 630, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 640, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 650, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 660, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 670, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 680, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 690, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 700, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 710, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 720, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 730, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 740, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 750, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 760, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 770, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 780, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 790, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 800, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 810, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 820, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 830, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 840, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 850, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 860, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 870, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 880, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 890, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 900, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 910, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 920, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 930, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 940, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 950, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 960, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 970, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 980, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 990, Validatioon Cost:: 2.301, Validation F1:: 0.113\n",
      "EPOCH:: 1000, Validatioon Cost:: 2.301, Validation F1:: 0.113\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Momentum),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax, Optimizer=Momentum),\n",
    "]\n",
    "\n",
    "train, test = compile_functions(layers)\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(1000):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.308, Validation F1:: 0.113\n",
      "EPOCH::  10, Validatioon Cost:: 2.305, Validation F1:: 0.113\n",
      "EPOCH::  20, Validatioon Cost:: 2.304, Validation F1:: 0.113\n",
      "EPOCH::  30, Validatioon Cost:: 2.303, Validation F1:: 0.104\n",
      "EPOCH::  40, Validatioon Cost:: 2.303, Validation F1:: 0.113\n",
      "EPOCH::  50, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH::  60, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH::  70, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH::  80, Validatioon Cost:: 2.303, Validation F1:: 0.113\n",
      "EPOCH::  90, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 100, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 110, Validatioon Cost:: 2.303, Validation F1:: 0.113\n",
      "EPOCH:: 120, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 130, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 140, Validatioon Cost:: 2.302, Validation F1:: 0.113\n",
      "EPOCH:: 150, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 160, Validatioon Cost:: 2.302, Validation F1:: 0.104\n",
      "EPOCH:: 170, Validatioon Cost:: 0.203, Validation F1:: 0.942\n",
      "EPOCH:: 180, Validatioon Cost:: 0.125, Validation F1:: 0.965\n",
      "EPOCH:: 190, Validatioon Cost:: 0.116, Validation F1:: 0.970\n",
      "EPOCH:: 200, Validatioon Cost:: 0.118, Validation F1:: 0.973\n",
      "EPOCH:: 210, Validatioon Cost:: 0.126, Validation F1:: 0.973\n",
      "EPOCH:: 220, Validatioon Cost:: 0.126, Validation F1:: 0.975\n",
      "EPOCH:: 230, Validatioon Cost:: 0.136, Validation F1:: 0.975\n",
      "EPOCH:: 240, Validatioon Cost:: 0.141, Validation F1:: 0.975\n",
      "EPOCH:: 250, Validatioon Cost:: 0.153, Validation F1:: 0.974\n",
      "EPOCH:: 260, Validatioon Cost:: 0.156, Validation F1:: 0.975\n",
      "EPOCH:: 270, Validatioon Cost:: 0.163, Validation F1:: 0.976\n",
      "EPOCH:: 280, Validatioon Cost:: 0.162, Validation F1:: 0.975\n",
      "EPOCH:: 290, Validatioon Cost:: 0.167, Validation F1:: 0.975\n",
      "EPOCH:: 300, Validatioon Cost:: 0.173, Validation F1:: 0.975\n",
      "EPOCH:: 310, Validatioon Cost:: 0.179, Validation F1:: 0.975\n",
      "EPOCH:: 320, Validatioon Cost:: 0.169, Validation F1:: 0.976\n",
      "EPOCH:: 330, Validatioon Cost:: 0.175, Validation F1:: 0.975\n",
      "EPOCH:: 340, Validatioon Cost:: 0.175, Validation F1:: 0.976\n",
      "EPOCH:: 350, Validatioon Cost:: 0.177, Validation F1:: 0.975\n",
      "EPOCH:: 360, Validatioon Cost:: 0.181, Validation F1:: 0.975\n",
      "EPOCH:: 370, Validatioon Cost:: 0.184, Validation F1:: 0.975\n",
      "EPOCH:: 380, Validatioon Cost:: 0.186, Validation F1:: 0.975\n",
      "EPOCH:: 390, Validatioon Cost:: 0.188, Validation F1:: 0.975\n",
      "EPOCH:: 400, Validatioon Cost:: 0.190, Validation F1:: 0.975\n",
      "EPOCH:: 410, Validatioon Cost:: 0.192, Validation F1:: 0.975\n",
      "EPOCH:: 420, Validatioon Cost:: 0.194, Validation F1:: 0.975\n",
      "EPOCH:: 430, Validatioon Cost:: 0.196, Validation F1:: 0.975\n",
      "EPOCH:: 440, Validatioon Cost:: 0.197, Validation F1:: 0.975\n",
      "EPOCH:: 450, Validatioon Cost:: 0.199, Validation F1:: 0.975\n",
      "EPOCH:: 460, Validatioon Cost:: 0.200, Validation F1:: 0.976\n",
      "EPOCH:: 470, Validatioon Cost:: 0.202, Validation F1:: 0.975\n",
      "EPOCH:: 480, Validatioon Cost:: 0.204, Validation F1:: 0.975\n",
      "EPOCH:: 490, Validatioon Cost:: 0.204, Validation F1:: 0.975\n",
      "EPOCH:: 500, Validatioon Cost:: 0.206, Validation F1:: 0.976\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.01\n",
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=AdaGrad, gamma=gamma),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax, Optimizer=AdaGrad, gamma=gamma),\n",
    "]\n",
    "\n",
    "train, test = compile_functions(layers)\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(500):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "\n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 2.327, Validation F1:: 0.100\n",
      "EPOCH::  10, Validatioon Cost:: 0.136, Validation F1:: 0.960\n",
      "EPOCH::  20, Validatioon Cost:: 0.099, Validation F1:: 0.975\n",
      "EPOCH::  30, Validatioon Cost:: 0.127, Validation F1:: 0.971\n",
      "EPOCH::  40, Validatioon Cost:: 0.116, Validation F1:: 0.978\n",
      "EPOCH::  50, Validatioon Cost:: 0.117, Validation F1:: 0.980\n",
      "EPOCH::  60, Validatioon Cost:: 0.128, Validation F1:: 0.980\n",
      "EPOCH::  70, Validatioon Cost:: 0.117, Validation F1:: 0.983\n",
      "EPOCH::  80, Validatioon Cost:: 0.109, Validation F1:: 0.983\n",
      "EPOCH::  90, Validatioon Cost:: 0.118, Validation F1:: 0.982\n",
      "EPOCH:: 100, Validatioon Cost:: 0.127, Validation F1:: 0.981\n",
      "EPOCH:: 110, Validatioon Cost:: 0.145, Validation F1:: 0.978\n",
      "EPOCH:: 120, Validatioon Cost:: 0.112, Validation F1:: 0.982\n",
      "EPOCH:: 130, Validatioon Cost:: 0.180, Validation F1:: 0.976\n",
      "EPOCH:: 140, Validatioon Cost:: 0.141, Validation F1:: 0.984\n",
      "EPOCH:: 150, Validatioon Cost:: 0.145, Validation F1:: 0.981\n",
      "EPOCH:: 160, Validatioon Cost:: 0.197, Validation F1:: 0.983\n",
      "EPOCH:: 170, Validatioon Cost:: 0.252, Validation F1:: 0.983\n",
      "EPOCH:: 180, Validatioon Cost:: 0.286, Validation F1:: 0.983\n",
      "EPOCH:: 190, Validatioon Cost:: 0.314, Validation F1:: 0.983\n",
      "EPOCH:: 200, Validatioon Cost:: 0.334, Validation F1:: 0.983\n",
      "EPOCH:: 210, Validatioon Cost:: 0.345, Validation F1:: 0.983\n",
      "EPOCH:: 220, Validatioon Cost:: 0.350, Validation F1:: 0.983\n",
      "EPOCH:: 230, Validatioon Cost:: 0.355, Validation F1:: 0.983\n",
      "EPOCH:: 240, Validatioon Cost:: 0.155, Validation F1:: 0.984\n",
      "EPOCH:: 250, Validatioon Cost:: 0.190, Validation F1:: 0.984\n",
      "EPOCH:: 260, Validatioon Cost:: 0.224, Validation F1:: 0.985\n",
      "EPOCH:: 270, Validatioon Cost:: 0.255, Validation F1:: 0.984\n",
      "EPOCH:: 280, Validatioon Cost:: 0.208, Validation F1:: 0.983\n",
      "EPOCH:: 290, Validatioon Cost:: 0.233, Validation F1:: 0.983\n",
      "EPOCH:: 300, Validatioon Cost:: 0.221, Validation F1:: 0.983\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid, Optimizer=Adam),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax, Optimizer=Adam),\n",
    "]\n",
    "\n",
    "train, test = compile_functions(layers)\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "for epoch in range(300):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "\n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
