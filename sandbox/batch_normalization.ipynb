{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "[Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/abs/1502.03167 \"[1502.03167] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 2: GeForce GTX TITAN X\n"
     ]
    }
   ],
   "source": [
    "import theano.sandbox.cuda\n",
    "theano.sandbox.cuda.use(\"gpu2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import numpy\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Random Seed\n",
    "rng = numpy.random.RandomState(1234)\n",
    "trng = RandomStreams(42)\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist_x, mnist_y = shuffle(mnist.data.astype(\"float32\")/255.0, mnist.target.astype(\"int32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, in_dim, out_dim, function, beta=0.0, gamma=1.0, eps=1e-5):\n",
    "        self.func = function\n",
    "        self.W = theano.shared(\n",
    "            rng.uniform(\n",
    "                low=-numpy.sqrt(6./(in_dim+out_dim)),\n",
    "                high=numpy.sqrt(6./(in_dim+out_dim)),\n",
    "                size=(in_dim, out_dim)\n",
    "            ).astype('float32'), name='W'\n",
    "        )\n",
    "        self.beta = theano.shared((numpy.zeros(in_dim) + beta).astype('float32'), name='beta')\n",
    "        self.gamma = theano.shared((numpy.zeros(in_dim) + gamma).astype('float32'), name='gamma')\n",
    "        self.eps = eps\n",
    "        self.params = [self.W, self.beta, self.gamma]\n",
    "        \n",
    "        self.avg_mean = theano.shared(numpy.zeros(in_dim).astype('float32'), name='avg_mean') # 入力（ミニバッチ）の各次元の平均の平均\n",
    "        self.avg_var = theano.shared(numpy.zeros(in_dim).astype('float32'), name='avg_var') # 入力（ミニバッチ）の各次元の分散の平均\n",
    "        self.N = theano.shared(numpy.float32(0), name='N') # 学習回数\n",
    "        \n",
    "        # 前回のパラメータ更新量 (P.52)\n",
    "        self.prev_dW = theano.shared(numpy.zeros((in_dim, out_dim)).astype('float32'), name='prev_dW')\n",
    "        self.prev_dbeta = theano.shared(numpy.zeros(in_dim).astype('float32'), name='prev_dbeta')\n",
    "        self.prev_dgamma = theano.shared(numpy.zeros(in_dim).astype('float32'), name='prev_dgamma')\n",
    "        self.prev_dparams = [self.prev_dW, self.prev_dbeta, self.prev_dgamma]\n",
    "\n",
    "    # We will be able to use theano.tensor.nnet.bn.batch_normalization since Theano v0.7.1\n",
    "    # http://deeplearning.net/software/theano/library/tensor/nnet/bn.html\n",
    "    def bn(self, x, mean, var):\n",
    "        '''\n",
    "        batch normalization\n",
    "    \n",
    "        :param x: 入力\n",
    "        :param mean: 平均\n",
    "        :param var: 分散\n",
    "        :return: xをbatch normalizationした値\n",
    "        '''\n",
    "        x_hat = (x - mean) / T.sqrt(var + self.eps)\n",
    "        y = self.gamma * x_hat + self.beta    \n",
    "        return y\n",
    "\n",
    "    def fprop(self, x):\n",
    "        '''\n",
    "        順伝播\n",
    "    \n",
    "        :param x: 入力\n",
    "        :return: レイヤーの出力\n",
    "        '''\n",
    "        self.mean = T.mean(x, axis=0)\n",
    "        self.var = T.var(x, axis=0)\n",
    "        \n",
    "        y = self.bn(x, self.mean, self.var)\n",
    "        z = self.func(T.dot(y, self.W))\n",
    "        self.z = z\n",
    "        return z\n",
    "\n",
    "    def predict(self, x, m):\n",
    "        '''\n",
    "        ネットワークパラメータを固定して推定する\n",
    "    \n",
    "        :param x: 入力\n",
    "        :param m: バッチサイズ (m > 1)\n",
    "        :return: 推定値（レイヤーの出力）\n",
    "        '''\n",
    "        var = self.avg_var * m / (m - 1)\n",
    "        y = self.bn(x, self.avg_mean, var)\n",
    "        z = self.func(T.dot(y, self.W))\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theano functionをコンパイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fprops(layers, x):\n",
    "    '''\n",
    "    ネットワーク全体の順伝播\n",
    "\n",
    "    :param layers: ネットワーク \n",
    "    :param x: 入力\n",
    "    :return: 出力層の出力\n",
    "    '''\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.fprop(z)    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(layers, x, m):\n",
    "    '''\n",
    "    学習済みネットワークを用いて推定する（ネットワーク全体の順伝播）\n",
    "\n",
    "    :param layers: ネットワーク \n",
    "    :param x: 入力\n",
    "    :param m: バッチサイズ\n",
    "    :return: 推定値（出力層の出力）\n",
    "    '''\n",
    "    z = x\n",
    "    for layer in layers:\n",
    "        z = layer.predict(z, m)    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cost Function (Negative Log Likelihood)\n",
    "def cross_entropy(y, d):\n",
    "    '''\n",
    "    交差エントロピーを計算する\n",
    "    See: (2.11) and #3.3\n",
    "\n",
    "    :param y: 出力層の出力\n",
    "    :param d: 目標出力\n",
    "    :return: 交差エントロピー\n",
    "    '''\n",
    "    # cf. http://deeplearning.net/tutorial/logreg.html#defining-a-loss-function\n",
    "    # cf. http://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing\n",
    "    return -T.mean(T.log(y)[T.arange(d.shape[0]), d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_with_momentum(params, gparams, prev_dparams, eps=0.01, mu=0.9):\n",
    "    '''\n",
    "    stochastic gradient descent with momentum\n",
    "    See: #3.2, #3.6.4 and P.52\n",
    "    \n",
    "    :param params: 更新するパラメータ\n",
    "    :param gparams: パラメータの勾配\n",
    "    :param eps: 学習率\n",
    "    :param mu: モメンタム\n",
    "    :return: 更新後のパラメータが格納されたOrderedDict\n",
    "    '''\n",
    "    updates = OrderedDict()\n",
    "    for param, gparam, prev_dparam in zip(params, gparams, prev_dparams):\n",
    "        updates[param] = param - eps * gparam + mu * prev_dparam\n",
    "        updates[prev_dparam] = - eps * gparam + mu * prev_dparam\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train(layers, lr=0.01):\n",
    "    '''\n",
    "    学習を行うtheano.functionを生成する\n",
    "\n",
    "    :param layers: 学習対象のネットワーク\n",
    "    :param lr: 学習率\n",
    "    :return: theano.function\n",
    "    '''\n",
    "    x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "    \n",
    "    ## Collect Parameters and Symbolic output\n",
    "    params = []\n",
    "    prev_dparams = []\n",
    "    for layer in layers:\n",
    "        params += layer.params\n",
    "        prev_dparams += layer.prev_dparams\n",
    "    \n",
    "    y = fprops(layers, x)\n",
    "    cost = cross_entropy(y, t)\n",
    "    \n",
    "    ## Get Gradient\n",
    "    gparams = T.grad(cost, params)\n",
    "    updates = sgd_with_momentum(params, gparams, prev_dparams, eps=lr)\n",
    "    \n",
    "    # Update stats for batch normalization\n",
    "    for layer in layers:\n",
    "        N = layer.N\n",
    "        avg_mean = layer.avg_mean\n",
    "        avg_var  = layer.avg_var\n",
    "        # 統計量を更新\n",
    "        updates[N] = N + 1\n",
    "        updates[avg_mean] = (avg_mean*N + layer.mean) / (N + 1)\n",
    "        updates[avg_var]  = (avg_var*N + layer.var) / (N + 1)\n",
    "    \n",
    "    ## Compile\n",
    "    train = theano.function([x,t], cost, updates=updates)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_test(layers, m):\n",
    "    '''\n",
    "    推定を行うtheano.functionを生成する\n",
    "\n",
    "    :param layers: 学習対象のネットワーク\n",
    "    :param m: バッチサイズ\n",
    "    :return: theano.function\n",
    "    '''\n",
    "    x, t = T.fmatrix(\"x\"), T.ivector(\"t\")\n",
    "\n",
    "    y = predict(layers, x, m)\n",
    "    cost = cross_entropy(y, t)\n",
    "\n",
    "    ## Compile\n",
    "    test = theano.function([x,t], [cost, T.argmax(y, axis=1)])\n",
    "    return test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実験"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, valid_x, train_y, valid_y = train_test_split(mnist_x, mnist_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 0.222, Validation F1:: 0.935\n",
      "EPOCH::  10, Validatioon Cost:: 0.086, Validation F1:: 0.976\n",
      "EPOCH::  20, Validatioon Cost:: 0.087, Validation F1:: 0.979\n",
      "EPOCH::  30, Validatioon Cost:: 0.087, Validation F1:: 0.980\n",
      "EPOCH::  40, Validatioon Cost:: 0.090, Validation F1:: 0.980\n",
      "EPOCH::  50, Validatioon Cost:: 0.089, Validation F1:: 0.981\n",
      "EPOCH::  60, Validatioon Cost:: 0.091, Validation F1:: 0.981\n",
      "EPOCH::  70, Validatioon Cost:: 0.093, Validation F1:: 0.981\n",
      "EPOCH::  80, Validatioon Cost:: 0.096, Validation F1:: 0.981\n",
      "EPOCH::  90, Validatioon Cost:: 0.094, Validation F1:: 0.982\n",
      "EPOCH:: 100, Validatioon Cost:: 0.095, Validation F1:: 0.981\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax),\n",
    "]\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "train = get_train(layers)\n",
    "test = get_test(layers, batch_size)\n",
    "\n",
    "for epoch in range(100):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 0.166, Validation F1:: 0.951\n",
      "EPOCH::  10, Validatioon Cost:: 0.100, Validation F1:: 0.974\n",
      "EPOCH::  20, Validatioon Cost:: 0.099, Validation F1:: 0.978\n",
      "EPOCH::  30, Validatioon Cost:: 0.105, Validation F1:: 0.979\n",
      "EPOCH::  40, Validatioon Cost:: 0.107, Validation F1:: 0.978\n",
      "EPOCH::  50, Validatioon Cost:: 0.090, Validation F1:: 0.981\n",
      "EPOCH::  60, Validatioon Cost:: 0.095, Validation F1:: 0.982\n",
      "EPOCH::  70, Validatioon Cost:: 0.094, Validation F1:: 0.983\n",
      "EPOCH::  80, Validatioon Cost:: 0.096, Validation F1:: 0.982\n",
      "EPOCH::  90, Validatioon Cost:: 0.095, Validation F1:: 0.983\n",
      "EPOCH:: 100, Validatioon Cost:: 0.100, Validation F1:: 0.982\n",
      "EPOCH:: 110, Validatioon Cost:: 0.104, Validation F1:: 0.981\n",
      "EPOCH:: 120, Validatioon Cost:: 0.103, Validation F1:: 0.982\n",
      "EPOCH:: 130, Validatioon Cost:: 0.099, Validation F1:: 0.982\n",
      "EPOCH:: 140, Validatioon Cost:: 0.102, Validation F1:: 0.983\n",
      "EPOCH:: 150, Validatioon Cost:: 0.105, Validation F1:: 0.983\n",
      "EPOCH:: 160, Validatioon Cost:: 0.103, Validation F1:: 0.984\n",
      "EPOCH:: 170, Validatioon Cost:: 0.102, Validation F1:: 0.983\n",
      "EPOCH:: 180, Validatioon Cost:: 0.102, Validation F1:: 0.984\n",
      "EPOCH:: 190, Validatioon Cost:: 0.103, Validation F1:: 0.983\n",
      "EPOCH:: 200, Validatioon Cost:: 0.103, Validation F1:: 0.983\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax),\n",
    "]\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "train = get_train(layers)\n",
    "test = get_test(layers, batch_size)\n",
    "\n",
    "for epoch in range(200):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7層"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH::   1, Validatioon Cost:: 0.209, Validation F1:: 0.938\n",
      "EPOCH::  10, Validatioon Cost:: 0.101, Validation F1:: 0.976\n",
      "EPOCH::  20, Validatioon Cost:: 0.100, Validation F1:: 0.978\n",
      "EPOCH::  30, Validatioon Cost:: 0.103, Validation F1:: 0.979\n",
      "EPOCH::  40, Validatioon Cost:: 0.102, Validation F1:: 0.981\n",
      "EPOCH::  50, Validatioon Cost:: 0.112, Validation F1:: 0.979\n",
      "EPOCH::  60, Validatioon Cost:: 0.112, Validation F1:: 0.981\n",
      "EPOCH::  70, Validatioon Cost:: 0.114, Validation F1:: 0.980\n",
      "EPOCH::  80, Validatioon Cost:: 0.100, Validation F1:: 0.981\n",
      "EPOCH::  90, Validatioon Cost:: 0.107, Validation F1:: 0.981\n",
      "EPOCH:: 100, Validatioon Cost:: 0.110, Validation F1:: 0.981\n",
      "EPOCH:: 110, Validatioon Cost:: 0.113, Validation F1:: 0.981\n",
      "EPOCH:: 120, Validatioon Cost:: 0.109, Validation F1:: 0.981\n",
      "EPOCH:: 130, Validatioon Cost:: 0.120, Validation F1:: 0.980\n",
      "EPOCH:: 140, Validatioon Cost:: 0.118, Validation F1:: 0.980\n",
      "EPOCH:: 150, Validatioon Cost:: 0.112, Validation F1:: 0.981\n",
      "EPOCH:: 160, Validatioon Cost:: 0.107, Validation F1:: 0.981\n",
      "EPOCH:: 170, Validatioon Cost:: 0.106, Validation F1:: 0.982\n",
      "EPOCH:: 180, Validatioon Cost:: 0.109, Validation F1:: 0.982\n",
      "EPOCH:: 190, Validatioon Cost:: 0.105, Validation F1:: 0.983\n",
      "EPOCH:: 200, Validatioon Cost:: 0.099, Validation F1:: 0.983\n",
      "EPOCH:: 210, Validatioon Cost:: 0.104, Validation F1:: 0.982\n",
      "EPOCH:: 220, Validatioon Cost:: 0.103, Validation F1:: 0.983\n",
      "EPOCH:: 230, Validatioon Cost:: 0.110, Validation F1:: 0.982\n",
      "EPOCH:: 240, Validatioon Cost:: 0.106, Validation F1:: 0.983\n",
      "EPOCH:: 250, Validatioon Cost:: 0.108, Validation F1:: 0.982\n",
      "EPOCH:: 260, Validatioon Cost:: 0.108, Validation F1:: 0.983\n",
      "EPOCH:: 270, Validatioon Cost:: 0.117, Validation F1:: 0.981\n",
      "EPOCH:: 280, Validatioon Cost:: 0.111, Validation F1:: 0.982\n",
      "EPOCH:: 290, Validatioon Cost:: 0.113, Validation F1:: 0.982\n",
      "EPOCH:: 300, Validatioon Cost:: 0.113, Validation F1:: 0.982\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    Layer(in_dim=784,  out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=1000, function=T.nnet.sigmoid),\n",
    "    Layer(in_dim=1000, out_dim=10,   function=T.nnet.softmax),\n",
    "]\n",
    "\n",
    "batch_size = 100\n",
    "nbatches = train_x.shape[0] // batch_size\n",
    "\n",
    "train = get_train(layers)\n",
    "test = get_test(layers, batch_size)\n",
    "\n",
    "for epoch in range(300):\n",
    "    train_x, train_y = shuffle(train_x, train_y)\n",
    "    for i in range(nbatches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        train(train_x[start:end], train_y[start:end])\n",
    "    \n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        valid_cost, pred = test(valid_x, valid_y)\n",
    "        print(\"EPOCH:: {:3d}, Validatioon Cost:: {:.3f}, Validation F1:: {:.3f}\".format(epoch+1,\n",
    "                                                                                     float(valid_cost),\n",
    "                                                                                     f1_score(valid_y, pred, average=\"micro\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
